{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e73a803-bb64-412b-abd8-4b85de2a0ee4",
   "metadata": {},
   "source": [
    "# Titanic competition with TensorFlow Decision Forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc4593c-bd1f-4b9f-b90b-181eeca47785",
   "metadata": {},
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddcf108-38ff-4e0a-92d4-345574467020",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_decision_forests as tfdf\n",
    "\n",
    "print(f\"Found TF-DF {tfdf.__version__}\")\n",
    "\n",
    "# If haven't installed the required packages, uncomment and run:\n",
    "# (若尚未安裝以下套件，請先執行:)\n",
    "# !pip install tensorflow tensorflow_decision_forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758f4d82-9d94-4a33-8347-3431b0769b87",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5b7cac-fefa-4439-a7b2-acfa7942616a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀取 Titanic 的訓練與測試資料集\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "serving_df = pd.read_csv(\"../data/test.csv\")\n",
    "\n",
    "# 檢視前 10 筆資料\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43061826-2a7b-4de7-aeae-71319ec75bba",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ceb7d-5567-4d9e-9f7e-6106661954c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df):     # 定義函式 preprocess，目的是對Titanic資料做資料清理與欄位擴充處理。\n",
    "    df = df.copy()      # 複製傳進來的 DataFrame，避免直接修改原始資料 (good practice)。\n",
    "\n",
    "# Name欄位處理: 清除雜訊符號\n",
    "    def normalize_name(x):\n",
    "        return \" \".join([v.strip(\",()[].\\\"'\") for v in x.split(\" \")])\n",
    "    \n",
    "# Ticket欄位處理: 拆成兩部分\n",
    "# 第一部分: ticket_number(x)取最後一段(通常是號碼)\n",
    "    def ticket_number(x):\n",
    "        return x.split()[-1]\n",
    "    \n",
    "#第二部分: ticket_item(x)取前面的識別代碼部分\n",
    "    def ticket_item(x):\n",
    "        items = x.split()\n",
    "        if len(items) == 1:\n",
    "            return \"NONE\"\n",
    "        return \"_\".join(items[:-1])\n",
    "\n",
    "# .apply: Apply應用這些函式到 DataFrame 欄位:  \n",
    "    df[\"Name\"] = df[\"Name\"].apply(normalize_name)\n",
    "    df[\"Ticket_number\"] = df[\"Ticket\"].apply(ticket_number)\n",
    "    df[\"Ticket_item\"] = df[\"Ticket\"].apply(ticket_item)\n",
    "# 回傳處理好的資料 (dataframe = df)    \n",
    "    return df\n",
    "\n",
    "#把 train_df 和 serving_df 各自傳進去 preprocess()函式，做完資料清理後得到兩份新的資料集。\n",
    "preprocessed_train_df = preprocess(train_df)\n",
    "preprocessed_serving_df = preprocess(serving_df)\n",
    "\n",
    "# 顯示前 5 筆資料:\n",
    "preprocessed_train_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2222a01-3d99-41e8-8d4d-2dbc3386db42",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = list(preprocessed_train_df.columns)\n",
    "input_features.remove(\"Ticket\")\n",
    "input_features.remove(\"PassengerId\")\n",
    "input_features.remove(\"Survived\")\n",
    "#input_features.remove(\"Ticket_number\")\n",
    "\n",
    "print(f\"Input features: {input_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd11fb1e-3a02-4515-8ff1-d877efff5d60",
   "metadata": {},
   "source": [
    "#### `.columns`是什麼意思?\n",
    "`.columns`是Pandas`DataFrame`的一個屬性，它會回傳這個資料表的**所有欄位名稱(column names)**，也就是CSV檔案的「表頭」。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b4a6f2-298e-4a6f-96cc-f2a41e8b8815",
   "metadata": {},
   "source": [
    "## Convert Pandas dataset to TensorFlow Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63672388-3433-449d-bc6f-24f13431899e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_names(features, labels=None):\n",
    "    \"\"\"Divite the names into tokens. TF-DF can consume text tokens natively.\"\"\"\n",
    "    features[\"Name\"] = tf.strings.split(features[\"Name\"])\n",
    "    return features, labels\n",
    "\n",
    "train_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_train_df, label=\"Survived\").map(tokenize_names)\n",
    "serving_ds = tfdf.keras.pd_dataframe_to_tf_dataset(preprocessed_serving_df).map(tokenize_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab80097-7489-43a1-b5c9-8d1c48082e85",
   "metadata": {},
   "source": [
    "### 第一段: 定義函數`tokenize_names`\n",
    "\n",
    "- `def tokenize_names(...)`  \n",
    "  這是定義一個函數(function)叫做`tokenize_names`，  \n",
    "  它的用途是: **將每一筆資料的Name欄位進行文字分詞(tokenize)**。  \n",
    "- `features`  \n",
    "  這是輸入的特徵(欄位資料)，在TensorFlow Dataset中，一筆資料會長得像一個字典(dictionary)，例如:  \n",
    "  ```\n",
    "  features = {\n",
    "      \"Name\": \"Braund, Mr. Owen Harris\",\n",
    "      \"Age\": 22,\n",
    "      \"Sex\": \"male\",\n",
    "      ... }\n",
    "  ```  \n",
    "- `labels=None`  \n",
    "  這是用來存放「標籤」(例如`Survived`)，  \n",
    "  預設是`None`，代表測試資料可能沒有標籤。這樣設計能通用於train/test兩種資料。  \n",
    "  \n",
    "- `features[\"Name\"]`  \n",
    "  代表從features中取出`Name`欄位的值，可能是一個句子，例如`\"Smith, Mrs. Emma\"`。  \n",
    "\n",
    "- `tf.strings.split(...)`  \n",
    "  這是TensorFlow的字串函式，會**把字串依照空格自動切成多個片段**(tokens)：  \n",
    "  ```  \n",
    "  tf.strings.split(\"Smith, Mrs. Emma\")  \n",
    "  → <tf.Tensor: shape=(3,), values=['Smith', 'Mrs.', 'Emma']  \n",
    "  ```  \n",
    "\n",
    "#### 第一段整行的意思: \n",
    "用空白切割每個名字，把原本的`\"Smith, Mrs. Emma\"`變成一個文字清單(tokens)，並**回寫進features[\"Name\"]**。  \n",
    "\n",
    "### 第二段: 建立`train_ds`訓練資料集\n",
    " \n",
    "- `tfdf.keras.pd_dataframe_to_tf_dataset(...)`  \n",
    "  這是TF-DF的API，作用是把Pandas的`DataFrame`轉成TensorFlow的`tf.data.Dataset`，這樣才能給模型用。  \n",
    "- `preprocessed_train_df`  \n",
    "  這是前面處理過的訓練資料(有做過`.apply(...)`)等前處理)。  \n",
    "- `label=\"Survived\"`  \n",
    "  告訴TF-DF: 這份資料的「標籤」欄位是`Survived`，模型會以這個欄位作為預測目標。  \n",
    "- `.map(tokenized_names)`  \n",
    "  這是TensorFlow的Dataset方法，會讓每一筆資料都經過`tokenize_names`函數處理一次 → 做名字切割。  \n",
    "\n",
    "### 第三段: 建立`serving_ds`訓練資料集\n",
    "- `preprocessed_serving_df`  \n",
    "  這是處理後的測試資料 (沒有`Survived`欄位)。  \n",
    "- **沒有label**  \n",
    "  因為我們沒有`Survived`可以提供，所以`label`參數不重要。  \n",
    "- `.map(tokenize_names`  \n",
    "  一樣對`Name`欄位進行分詞處理。\n",
    "\n",
    "### 總結邏輯流程:\n",
    "|步驟|說明|\n",
    "|:-|:-|\n",
    "|`tokenize_names()`|處理每一筆資料，把Name拆成token|\n",
    "|`.map(...)`|套用到整個Dataset|\n",
    "|`pd_dataframe_to_tf_dataset(...)`|把Pandas資料匡轉成TF-DF能用的Dataset格式|\n",
    "|`train_ds / serving_ds`|建立兩個TensorFlow Dataset，後面會傳給模型訓練與預測使用|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7915897-54e4-4857-bd8d-ff220bb7aa8e",
   "metadata": {},
   "source": [
    "## Train model with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c833a7c-6b4b-4c9c-806b-341bf18c1216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第1部分: 建立Gradient Boosted Trees模型\n",
    "model = tfdf.keras.GradientBoostedTreesModel(\n",
    "    verbose=0,    # Very few logs (關閉大部分訓練過程的訊息，畫面會比較乾淨)\n",
    "    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
    "    exclude_non_specified_features=True,   #Only use the features in \"features\" (只使用指定的欄位)\n",
    "    random_seed=1234,\n",
    ")\n",
    "\n",
    "# 第2部分: 訓練模型與評估表現\n",
    "model.fit(train_ds)\n",
    "\n",
    "self_evaluation = model.make_inspector().evaluation()\n",
    "print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")\n",
    "\n",
    "# Accuracy: 模型預測對的比例 (準確率)\n",
    "# Loss: 損失函數的數值，越小代表預測越準"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f12a644a-93f0-44d5-8aef-8e23bee7efa0",
   "metadata": {},
   "source": [
    "### 語法說明:\n",
    "|問題|是什麼|用來做什麼|\n",
    "|:-|:-|:-|\n",
    "|1. `GradientBoostedTreesModel()`|模型類型|建立GBT|\n",
    "|2. `FeatureUsage()`|特徵指定物件|告訴模型要用哪些欄位|\n",
    "|3. `exclude_non_specified_fetures`|選擇性參數|是否排除未指定的欄位|\n",
    "|4. `random_seed`|隨機種子|讓結果可重現|\n",
    "|5. `make_inspector().evaluation()`|模型評估|查看準確率與損失值|\n",
    "|6. `.accuracy`/`.loss`|評估指標|評估訓練表現|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddf33be-9e9b-47fe-b0fa-cc9b07ebe24b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "543b6914-6395-4569-baf2-a029a2b8e603",
   "metadata": {},
   "source": [
    "## Train model with improved default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d342a98-f6e5-486c-9f9e-902df6703d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tfdf.keras.GradientBoostedTreesModel(\n",
    "    verbose=0,    #Very few logs\n",
    "    features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
    "    exclude_non_specified_features=True,   # Only use the features in \"features\"\n",
    "\n",
    "    #num_trees=2000,\n",
    "\n",
    "    # Only for GBT.\n",
    "    # A bit slower, but great to understand the model.\n",
    "    # compute_permutation_variable_importance=True,\n",
    "\n",
    "    # Change the default hyper-parameters\n",
    "    # hyperparameter_template=\"benchmark_rank1@v1\",\n",
    "\n",
    "    #num_trees=1000,\n",
    "    #tuner=tuner\n",
    "\n",
    "    min_examples=1,\n",
    "    categorical_algorithm=\"RANDOM\",\n",
    "    #max_depth=4,\n",
    "    shrinkage=0.05,\n",
    "    #num_candidate_attributes_ratio=0.2,\n",
    "    split_axis=\"SPARSE_OBLIQUE\",\n",
    "    sparse_oblique_normalization=\"MIN_MAX\",\n",
    "    sparse_oblique_num_projections_exponent=2.0,\n",
    "    num_trees=2000,\n",
    "    #validation_ratio=0.0,\n",
    "    random_seed=1234,\n",
    "\n",
    ")\n",
    "model.fit(train_ds)\n",
    "\n",
    "self_evaluation = model.make_inspector().evaluation()\n",
    "print(f\"Accuracy: {self_evaluation.accuracy} Loss:{self_evaluation.loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efb2531-f94a-4698-a8c1-7cd72bb87636",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18491883-960b-4782-95df-33a9b1b1081d",
   "metadata": {},
   "source": [
    "### 小結: 這段summary的用途是?\n",
    "|想了解什麼|看哪裡?|\n",
    "|:-|:-|\n",
    "|使用哪些欄位?|Input Features|\n",
    "|哪些欄位最重要?|Variable Importance|\n",
    "|模型有幾棵樹?深度?|Number of trees、Depth by leafs|\n",
    "|訓練過程的準確率、損失|Training logs|"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4b2f2b-c823-48a0-8acc-f67ebffa086b",
   "metadata": {},
   "source": [
    "## Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1ebac-57ab-4c71-9dc8-a654944f2e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_to_kaggle_format(model, threshold=0.5):\n",
    "    proba_survive = model.predict(serving_ds, verbose=0)[:,0]\n",
    "    return pd.DataFrame({\n",
    "        \"PassengerId\": serving_df[\"PassengerId\"],\n",
    "        \"Survived\": (proba_survive >= threshold).astype(int)\n",
    "    })\n",
    "\n",
    "def make_submission(kaggle_predictions):\n",
    "    import os\n",
    "    if os.path.exists(\"/kagle/working\"):\n",
    "        path=\"/kaggle/working/submission.csv\"\n",
    "    else:\n",
    "        path=\"../output/submission.csv\"\n",
    "    kaggle_predictions.to_csv(path, index=False)\n",
    "    print(f\"Submission exported to {path}\")\n",
    "\n",
    "kaggle_predictions = prediction_to_kaggle_format(model)\n",
    "make_submission(kaggle_predictions)\n",
    "!head ../output/submission.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea1e7e6-986f-45ac-b2ce-eb81c733272d",
   "metadata": {},
   "source": [
    "## Training a model with hyperparameter tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b93baba-d833-461f-843f-42df7d37ba5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = tfdf.tuner.RandomSearch(num_trials=1000)\n",
    "tuner.choice(\"min_examples\", [2, 5, 7, 10])\n",
    "tuner.choice(\"categorical_algorithm\", [\"CART\", \"RANDOM\"])\n",
    "\n",
    "local_search_space = tuner.choice(\"growing_strategy\", [\"LOCAL\"])\n",
    "local_search_space.choice(\"max_depth\", [3, 4, 5, 6, 8])\n",
    "\n",
    "global_search_space = tuner.choice(\"growing_strategy\", [\"BEST_FIRST_GLOBAL\"], merge=True)\n",
    "global_search_space.choice(\"max_num_nodes\", [16, 32, 64, 128, 256])\n",
    "\n",
    "#tuner.choice(\"use_hessian_gain\", [True, False])\n",
    "tuner.choice(\"shrinkage\", [0.02, 0.05, 0.10, 0.15])\n",
    "tuner.choice(\"num_candidate_attributes_ratio\", [0.2, 0.5, 0.9, 1.0])\n",
    "\n",
    "tuner.choice(\"split_axis\", [\"AXIS_ALIGNED\"])\n",
    "oblique_space = tuner.choice(\"split_axis\", [\"SPARSE_OBLIQUE\"], merge=True)\n",
    "oblique_space.choice(\"sparse_oblique_normalization\",\n",
    "                     [\"NONE\", \"STANDARD_DEVIATION\", \"MIN_MAX\"])\n",
    "oblique_space.choice(\"sparse_oblique_weights\", [\"BINARY\", \"CONTINUOUS\"])\n",
    "oblique_space.choice(\"sparse_oblique_num_projections_exponent\", [1.0, 1.5])\n",
    "\n",
    "# Tune the model. Notice the `tuner=tuner`.\n",
    "tuned_model = tfdf.keras.GradientBoostedTreesModel(tuner=tuner)\n",
    "tuned_model.fit(train_ds, verbose=0)\n",
    "\n",
    "tuned_self_evaluation = tuned_model.make_inspector().evaluation()\n",
    "print(f\"Accuracy: {tuned_self_evaluation.accuracy} Loss: {tuned_self_evaluation.loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfcc577f-8bdf-4e36-8890-972338bb1f93",
   "metadata": {},
   "source": [
    "## Making an ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60ae6a7f-18b6-4624-97a3-6f74f8548f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = None\n",
    "num_predictions = 0\n",
    "\n",
    "for i in range(100):\n",
    "    print(f\"i:{i}\")\n",
    "    # Possible models: GradientBoostedTreesModel or RandomForestModel\n",
    "    model = tfdf.keras.GradientBoostedTreesModel(\n",
    "        verbose=0,   # Very few logs\n",
    "        features=[tfdf.keras.FeatureUsage(name=n) for n in input_features],\n",
    "        exclude_non_specified_features=True,  # Only use the features in \"features\"\n",
    "\n",
    "        #min_examples=1,\n",
    "        #categorical_algorithm=\"RANDOM\",\n",
    "        ##max_depth=4\n",
    "        #shrinkage=0.05,\n",
    "        ##num_candidate_attributes_ratio=0.2,\n",
    "        #split_axis=\"SPARSE_OBLIQUE\",\n",
    "        #sparse_oblique_normalization=\"MIN_MAX\",\n",
    "        #sparse_oblique_num_projections_exponent=2.0,\n",
    "        #num_trees=2000,\n",
    "        ##validation_ratio=0.0\n",
    "        random_seed=i,\n",
    "        honest=True,\n",
    "    )\n",
    "    model.fit(train_ds)\n",
    "\n",
    "    sub_predictions = model.predict(serving_ds, verbose=0) [:,0]\n",
    "    if predictions is None:\n",
    "        predictions = sub_predictions\n",
    "    else:\n",
    "        predictions += sub_predictions\n",
    "    num_predictions += 1\n",
    "\n",
    "predictions/=num_predictions\n",
    "\n",
    "kaggle_predictions = pd.DataFrame({\n",
    "        \"PassengerId\": serving_df[\"PassengerId\"],\n",
    "        \"Survived\": (predictions >= 0.5).astype(int)\n",
    "    })\n",
    "\n",
    "make_submission(kaggle_predictions)\n",
    "\n",
    "import os\n",
    "if os.path.exists(\"/kaggle/working\"):\n",
    "    path = \"/kaggle/working/submission.csv\"\n",
    "else:\n",
    "    path = \"../output/submission.csv\"\n",
    "kaggle_predictions.to_csv(path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e74548-d485-461e-8122-5873d449e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = tuned_model.make_inspector().evaluation()\n",
    "print(f\"Accuracy: {evaluation.accuracy}\")\n",
    "print(f\"Loss: {evaluation.loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aeb6b9-6c64-437d-bfa7-75a912a70597",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
